# Name of your application. Used to uniquely configure containers.
service: workshop-kamal2

# Name of the container image.
image: alexander.borisov/workshop-kamal2

# Deploy to these servers.
servers:
  web:
    - 192.168.0.2
  job:
    hosts:
      - 192.168.0.4
    cmd: bin/jobs
    env:
      clear:
        START_METRICS_SERVER: 1
  cron:
    hosts:
      - 192.168.0.4
    cmd: bin/jobs -c config/recurring.yml

# Enable SSL auto certification via Let's Encrypt and allow for multiple apps on a single web server.
# Remove this section when using multiple web servers and ensure you terminate SSL at your load balancer.
#
# Note: If using Cloudflare, set encryption mode in SSL/TLS setting to "Full" to enable CF-to-app encryption.
proxy:
  ssl: true
  host: kamal-testbed.online
  # Proxy connects to your container on port 80 by default.
  # app_port: 3000

# Credentials for your image host.
registry:
  # Specify the registry server, if you're not using Docker Hub
  # server: registry.digitalocean.com / ghcr.io / ...
  server: registry.gitlab.com
  username: alexander.borisov

  # Always use an access token rather than real password (pulled from .kamal/secrets).
  password:
    - KAMAL_REGISTRY_PASSWORD

# Configure builder setup.
builder:
  arch: amd64

# Inject ENV variables into containers (secrets come from .kamal/secrets).
#
env:
  clear:
    RAILS_LOG_LEVEL: debug
    DB_HOST: 192.168.0.3
  secret:
    - RAILS_MASTER_KEY
    - WORKSHOP_KAMAL2_DATABASE_PASSWORD
    - S3_ACCESS_KEY_ID
    - S3_SECRET_ACCESS_KEY

# Aliases are triggered with "bin/kamal <alias>". You can overwrite arguments on invocation:
# "bin/kamal logs -r job" will tail logs from the first server in the job section.
#
aliases:
  shell: app exec --interactive --reuse "bash"
  console: app exec --interactive --reuse "bin/rails c"
  watch-logs: app logs -f

# Use a different ssh user than root
#
ssh:
  proxy: root@87.228.32.111

# Use a persistent storage volume.
#
volumes:
  - "storage:/rails/storage"

# Bridge fingerprinted assets, like JS and CSS, between versions to avoid
# hitting 404 on in-flight requests. Combines all files from new and old
# version inside the asset_path.
#
asset_path: /rails/public/assets

# Configure rolling deploys by setting a wait time between batches of restarts.
#
# boot:
#   limit: 10 # Can also specify as a percentage of total hosts, such as "25%"
#   wait: 2

# Use accessory services (secrets come from .kamal/secrets).
#
accessories:
  db:
    image: postgres:16.1
    host: 192.168.0.3
    port: 5432
    env:
      clear:
        POSTGRES_USER: workshop_kamal2
      secret:
        - POSTGRES_PASSWORD
    directories:
      - data:/var/lib/postgresql/data

  # To access Minio dashboard:
  # ssh -A -L 9001:192.168.0.5:9001 root@kamal-testbed.online
  # http://localhost:9001
  #
  # Host s3.kamal-testbed
  #   HostName 192.168.0.5
  #   ProxyJump kamal-testbed.online
  #   User root
  #   IdentityFile ~/.ssh/id_rsa
  #   Port 22
  s3:
    host: 192.168.0.5
    image: bitnami/minio:latest
    port: 9000
    options:
      publish: 9001:9001
    env:
      clear:
        MINIO_DEFAULT_BUCKETS: workshop-kamal-production
        MINIO_ROOT_USER: minioadmin
        MINIO_ROOT_PASSWORD: minioadmin
        MINIO_SITE_REGION: us-east-1
        MINIO_SERVER_URL: https://s3.kamal-testbed.online
    directories:
      - data:/bitnami/minio/data

  # https://grafana.com/grafana/dashboards/1860-node-exporter-full/
  node_exporter:
    image: prom/node-exporter
    port: 9100
    hosts:
      - 192.168.0.2
      - 192.168.0.4
      - 192.168.0.3
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    cmd: "--path.rootfs=/rootfs --collector.systemd --collector.processes"

  otel_collector:
    image: otel/opentelemetry-collector:0.100.0
    port: 9394
    files:
      - config/deploy/otel_collector.yml:/etc/otelcol/config.yaml
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    options:
      user: 0
    hosts:
      - 192.168.0.2
      - 192.168.0.4
      - 192.168.0.3

  # To access Prometheus dashboard via SSH tunnel:
  # ssh -L 9090:192.168.0.6:9090 root@kamal-testbed.online
  # http://localhost:9090
  prometheus:
    image: prom/prometheus
    host: 192.168.0.6
    port: 9100
    files:
      - ./config/deploy/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    volumes:
      - prometheus-data:/prometheus
    cmd: "--config.file=/etc/prometheus/prometheus.yml"
    options:
      publish:
        - "9090:9090"

  # kamal server exec "docker exec kamal-proxy kamal-proxy deploy grafana --host grafana.kamal-testbed.online --target 192.168.0.6:3000 --tls --health-check-path /api/health" -h 192.168.0.2
  # kamal server exec "docker exec kamal-proxy kamal-proxy list" -h 192.168.0.2
  # https://grafana.com/grafana/dashboards/12342-puma/ + https://github.com/yabeda-rb/yabeda-puma-plugin
  # https://grafana.com/grafana/dashboards/17303 + https://github.com/Fullscript/yabeda-activejob
  grafana:
    image: grafana/grafana
    host: 192.168.0.6
    port: 3000
    volumes:
      - grafana_db:/var/lib/grafana
    env:
      clear:
        GF_SERVER_ROOT_URL: https://grafana.kamal-testbed.online
      secret:
        - GF_SECURITY_ADMIN_PASSWORD

#   db:
#     image: mysql:8.0
#     host: 192.168.0.2
#     port: 3306
#     env:
#       clear:
#         MYSQL_ROOT_HOST: '%'
#       secret:
#         - MYSQL_ROOT_PASSWORD
#     files:
#       - config/mysql/production.cnf:/etc/mysql/my.cnf
#       - db/production.sql:/docker-entrypoint-initdb.d/setup.sql
#     directories:
#       - data:/var/lib/mysql
#   redis:
#     image: valkey/valkey:8
#     host: 192.168.0.2
#     port: 6379
#     directories:
#       - data:/data
